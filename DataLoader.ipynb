{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "import socket\n",
    "from torchtext.data import ReversibleField, BucketIterator\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "path = '/media/ubuntu/1TO/DTU/courses/DeepLearning/DeepLearning_summarization/SampleData/train_short_unique_countries.csv'\n",
    "path_val = '/media/ubuntu/1TO/DTU/courses/DeepLearning/DeepLearning_summarization/SampleData/validation_short_unique_countries.csv'\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, include_lengths=True)\n",
    "#    LABEL = data.Field(init_token='<bos>', eos_token='<eos>', sequential=True)\n",
    "train_set = data.TabularDataset(path, 'CSV', fields=[('data', TEXT), ('label', TEXT)], skip_header=True ) \n",
    "validation_set = data.TabularDataset(path_val, 'CSV', fields=[('data', TEXT), ('label', TEXT)], skip_header=True ) \n",
    "\n",
    "TEXT.build_vocab(train_set)#, vectors=\"glove.6B.\"+str(glove_dim)+\"d\")\n",
    "#    LABEL.vocab = TEXT.vocab\n",
    "vocab = copy.deepcopy(TEXT.vocab)\n",
    "\n",
    "\n",
    "#This is the glove embedding for every word in vocab, we dont need to load it into memory\n",
    "#w = embed.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "#Data loader iterator\n",
    "dataset_iter = data.Iterator(train_set, batch_size=BATCH_SIZE, device=device,\n",
    "        train=True, shuffle=False, repeat=False, sort=True, sort_key=lambda x: len(x.data))\n",
    "dataset_iter_train_val = data.Iterator(train_set, batch_size=1, device=device,\n",
    "        train=True, shuffle=False, repeat=False, sort=False)\n",
    "\n",
    "dataset_iter_val = data.Iterator(validation_set, batch_size=1, device=device,\n",
    "        train=True, shuffle=True, repeat=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display(x):\n",
    "    s = ' '.join([vocab.itos[i] for i in x if vocab.itos[i] not in ['<bos>', '<pad>']])\n",
    "    return s.split('<eos>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
      "torch.Size([7, 16])\n",
      "Data\n",
      "14 your sleep 15 nature \n",
      "Labels\n",
      "tensor([4, 5, 5, 6, 6, 7, 4, 6, 4, 5, 6, 4, 6, 5, 5, 5])\n",
      "torch.Size([7, 16])\n",
      "14 15 \n"
     ]
    }
   ],
   "source": [
    "for batchData in dataset_iter:\n",
    "    print(batchData.data[1])\n",
    "    print(batchData.data[0].size())\n",
    "    print('Data')\n",
    "    print(display(batchData.data[0][:,0]))\n",
    "    print('Labels')\n",
    "    print(batchData.label[1])\n",
    "    print(batchData.label[0].size())\n",
    "    print(display(batchData.label[0][:,0]))\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointerGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(44, 20)\n",
    "        self.encoder = nn.LSTM(input_size=20, hidden_size=100)\n",
    "        self.decoder = nn.LSTM(input_size=20, hidden_size=100)\n",
    "        self.out = nn.Linear(in_features=100, out_features=44)\n",
    "        self.attention = nn.Linear(in_features=100+100, out_features=50, bias=True)\n",
    "        self.att_out = nn.Linear(in_features=50, out_features=1, bias=False)\n",
    "        self.linear_in = nn.Linear(in_features=100, out_features=100, bias=True)\n",
    "        self.linear_vocab1 = nn.Linear(in_features = 100+100, out_features=50, bias=True)\n",
    "        self.linear_vocab2 = nn.Linear(in_features=50, out_features=44, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.linear_pgen = nn.Linear(in_features=100+100+20, out_features=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,y, x=None,  hid_enc=None, cell_enc=None, x_length=None, out_enc=None, real_x=None):\n",
    "        batch_size = y.size()[1]\n",
    "        big_vocab_size = torch.max(real_x).item()\n",
    "        if not x is None:\n",
    "            x_emb = self.embedding(x)\n",
    "            if x_length is not None:\n",
    "                # Lengths data is wrapped inside a Tensor.\n",
    "                lengths_list = x_length.view(-1).tolist()\n",
    "                packed_emb = pack(x_emb, lengths_list)\n",
    "\n",
    "            out_enc, (hid_enc, cell_enc) = self.encoder(packed_emb)\n",
    "            hid_enc = hid_enc\n",
    "            cell_enc = cell_enc\n",
    "            if x_length is not None:\n",
    "                out_enc = unpack(out_enc)[0]\n",
    "        \n",
    "        y_emb = self.embedding(y)\n",
    "        \n",
    "        out_enc_ = out_enc\n",
    "        out_dec, (hid_dec, cell_dec) = self.decoder(y_emb, (hid_enc, cell_enc))\n",
    "        \n",
    "        tgt_len, batch_size, tgt_dim = out_dec.size()\n",
    "        src_len = out_enc.size()[0]\n",
    "        out_dec_ = out_dec.view(batch_size * tgt_len, tgt_dim)\n",
    "        out_dec_ = self.linear_in(out_dec_)\n",
    "        out_dec_ = out_dec_.view(batch_size, tgt_len, tgt_dim)\n",
    "        out_enc = out_enc.permute([1,2,0])\n",
    "        # (batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)\n",
    "        align = torch.bmm(out_dec_, out_enc)\n",
    "        mask = sequence_mask(x_length, max_len=align.size(-1))\n",
    "        mask = mask.unsqueeze(1)  # Make it broadcastable.\n",
    "\n",
    "        align.masked_fill_(1 - mask, -float('inf'))\n",
    "        \n",
    "        align_vectors = F.softmax(align.view(batch_size*tgt_len, src_len), -1)\n",
    "        align_vectors = align_vectors.view(batch_size, tgt_len, src_len)\n",
    "        out_enc = out_enc.permute([0,2,1])\n",
    "        c = torch.bmm(align_vectors, out_enc)\n",
    "    \n",
    "        in_pvocab = torch.cat((out_dec, c.permute([1,0,2])), dim=2).view(-1, 200)\n",
    "        pvocab = self.linear_vocab2(self.linear_vocab1(in_pvocab)).view(-1, batch_size, 44)\n",
    "        p_vocab = self.softmax(pvocab)\n",
    "        \n",
    "        in_pgen = torch.cat((out_dec, c.permute([1,0,2]), y_emb), dim=2).view(-1, 220)\n",
    "        pgen = self.sigmoid(self.linear_pgen(in_pgen)).view(-1, batch_size,1)\n",
    "        p_vocab = pgen * p_vocab\n",
    "        if big_vocab_size > 44:\n",
    "            p_vocab = torch.cat((p_vocab, torch.zeros(tgt_len,batch_size,big_vocab_size-44)), dim=2)\n",
    "        \n",
    "                            \n",
    "        return p_vocab, hid_dec, cell_dec, out_enc_\n",
    "    \n",
    "model = PointerGenerator()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion = nn.NLLLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y, target):\n",
    "    return torch.mean((y==target).float())\n",
    "def label_mask(y):\n",
    "    \"\"\"y is the label tensor (no glove embedding)\n",
    "       returns the mask to use for negating the padding\"\"\"\n",
    "    mask = (y != vocab.stoi['<pad>'])\n",
    "    return mask.float()\n",
    "def compute_loss(y, t, mask=True):\n",
    "    \"\"\"\n",
    "    predictions = y[:-1].permute([1,2,0])\n",
    "    target = t[1:].permute([1,0])\n",
    "    loss = criterion(predictions, target)\"\"\"\n",
    "    batch_size = y.size()[1]\n",
    "    predictions = y[:-1].contiguous()\n",
    "    predictions=predictions.view(-1,predictions.size()[2])\n",
    "\n",
    "    target = t[1:].view(-1)\n",
    "    loss = criterion(torch.log(predictions), target).view(-1,batch_size)\n",
    "    if mask:\n",
    "        y_mask = label_mask(t[1:])\n",
    "        loss_mask = torch.sum(y_mask * loss, dim=0)\n",
    "        mean_loss = loss_mask/torch.sum(y_mask,dim=0)\n",
    "        final_loss = torch.mean(mean_loss)\n",
    "        return final_loss\n",
    "    else:\n",
    "        return torch.mean(loss)\n",
    "def sequence_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a boolean mask from sequence lengths.\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max()\n",
    "    return (torch.arange(0, max_len)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))\n",
    "def filter_unk(x, vocab_size):\n",
    "    return x * (x <= vocab_size+3).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Target 63 out of bounds at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/THNN/generic/ClassNLLCriterion.c:56",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1cdbac27c1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfinal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-5a972d0dedb4>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, t, mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Target 63 out of bounds at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/THNN/generic/ClassNLLCriterion.c:56"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    model.train()\n",
    "    mean_losses = []\n",
    "    accs = []\n",
    "    for batchData in dataset_iter:\n",
    "        real_x = batchData.data[0]\n",
    "        x = filter_unk(real_x, 40)\n",
    "        real_y = batchData.label[0]\n",
    "        y = filter_unk(real_y, 40)\n",
    "        out, _, _ ,_= model(y, x, x_length=batchData.data[1], real_x=real_x)\n",
    "        y_hat = torch.argmax(out,-1)\n",
    "        accuracy = acc(y_hat[:-1], y[1:])\n",
    "        \n",
    "        final_loss = compute_loss(out, real_y)\n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_losses.append(final_loss)\n",
    "        accs.append(accuracy)\n",
    "    print('EPOCH ', k+1, ' Train: mean loss: ', torch.mean(torch.stack(mean_losses)).item(), 'mean acc: ', torch.mean(torch.stack(accs)).item())\n",
    "mean_losses = []\n",
    "accs = []    \n",
    "model.eval()\n",
    "for batchData in dataset_iter_val:\n",
    "    out, _, _ ,_= model(batchData.label[0], batchData.data[0], x_length=batchData.data[1], real_x=real_x)\n",
    "    y_hat = torch.argmax(out,-1)\n",
    "    accuracy = acc(y_hat[:-1], batchData.label[0][1:])\n",
    "    loss = compute_loss(out, batchData.label[0])\n",
    "    mean_loss = torch.mean(loss)\n",
    "    mean_losses.append(mean_loss)\n",
    "    accs.append(accuracy)\n",
    "print('Validation TF: mean loss: ', torch.mean(torch.stack(mean_losses)).item(), 'mean acc: ', torch.mean(torch.stack(accs)).item())\n",
    "accs=[]\n",
    "lines_y = []\n",
    "lines_t = []\n",
    "for batchData in dataset_iter_val:\n",
    "    x =  batchData.data[0]\n",
    "    y = batchData.label[0]\n",
    "    out, hid_dec , cell_dec, out_enc = model(y[0:1,:], x=x, x_length=batchData.data[1])\n",
    "    w_hat = torch.argmax(out,-1)\n",
    "    out_sequence = [w_hat]\n",
    "    for i in range(21):\n",
    "        out, hid_dec, cell_dec,_ = model(w_hat, x=None, hid_enc=hid_dec, cell_enc=cell_dec, x_length=batchData.data[1], out_enc=out_enc)\n",
    "        w_hat = torch.argmax(out,-1)\n",
    "        out_sequence.append(w_hat)\n",
    "    y_hat = torch.stack(out_sequence).squeeze()\n",
    "    y_true = batchData.label[0][1:].squeeze()\n",
    "    accuracy = acc(y_hat[:y_true.size()[0]], y_true)\n",
    "    accs.append(accuracy)\n",
    "    lines_y.append(display(y_hat))\n",
    "    lines_t.append(display(y_true))\n",
    "line_y = '\\n'.join(lines_y)\n",
    "line_t = '\\n'.join(lines_t) \n",
    "y_file = open('output', 'w')\n",
    "y_file.write(line_y)\n",
    "y_file.close()\n",
    "t_file = open('reference', 'w')\n",
    "t_file.write(line_t)\n",
    "t_file.close()\n",
    "bleu_score = str(subprocess.Popen(\"perl multi-bleu.perl reference < output\", shell=True, stdout=subprocess.PIPE).stdout.read()).split(' ')[2][:-1]\n",
    "print('BLEU score:', bleu_score)\n",
    "print('Validation non TF: mean acc: ', torch.mean(torch.stack(accs)).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(real_x).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_iter_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mask(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
